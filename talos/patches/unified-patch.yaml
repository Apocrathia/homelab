---
# yaml-language-server: $schema=https://raw.githubusercontent.com/siderolabs/talos/v1.11.3/pkg/machinery/config/schemas/config.schema.json
machine:
  features:
    rbac: true
    kubernetesTalosAPIAccess:
      allowedKubernetesNamespaces:
        - system-upgrade
      allowedRoles:
        - os:admin
      enabled: true
  kubelet:
    extraArgs:
      rotate-server-certificates: "false"
      feature-gates: "MemoryQoS=true"
    extraMounts:
      # Longhorn https://longhorn.io/docs/1.9.0/advanced-resources/os-distro-specific/talos-linux-support/
      - destination: /var/lib/longhorn
        type: bind
        source: /var/lib/longhorn
        options:
          - bind
          - rshared
          - rw
      # Talos UserVolumes
      - destination: /var/mnt
        type: bind
        source: /var/mnt
        options:
          - bind
          - rshared
          - rw
  sysctls:
    # Completely disable swap for etcd performance
    vm.swappiness: "0" # Disable swapping completely
    vm.vfs_cache_pressure: "50" # Reduce VFS cache pressure
    # Network optimizations
    net.core.somaxconn: "32768"
    net.core.netdev_max_backlog: "16384"
    user.max_user_namespaces: "10000"
  network:
    interfaces:
      - interface: eth0
        dhcp: true
        vip:
          ip: "10.100.1.8"
  install:
    disk: /dev/sda
    wipe: true
    # renovate: datasource=github-releases depName=siderolabs/talos
    image: factory.talos.dev/metal-installer/8b583b5320ff41bf40bd1aea9287923ada45e32a1e2ba0045cb3f6758920ef4a:v1.12.0
    # https://factory.talos.dev/?arch=amd64&board=undefined&cmdline=net.ifnames%3D0&cmdline-set=true&extensions=-&extensions=siderolabs%2Famd-ucode&extensions=siderolabs%2Fiscsi-tools&extensions=siderolabs%2Fqemu-guest-agent&extensions=siderolabs%2Futil-linux-tools&platform=metal&secureboot=undefined&target=metal&version=1.11.2
  time:
    servers:
      - pool.ntp.org
cluster:
  clusterName: home
  allowSchedulingOnControlPlanes: true
  controlPlane:
    endpoint: https://kubernetes.apocrathia.com:6443
  apiServer:
    certSANs:
      - "10.100.1.8"
      - "kubernetes.apocrathia.com"
    # CRD Controller Specific Optimizations
    extraArgs:
      # Event garbage collection - 1 hour TTL
      event-ttl: "1h"
      # Valid API Server flags for CRD optimization
      max-requests-inflight: "800" # 2x increase from default 400
      max-mutating-requests-inflight: "400" # 2x increase from default 200
      request-timeout: "120s" # 2x increase from default 60s
      watch-cache-sizes: "customresourcedefinitions#500,secrets#1000,configmaps#1000,persistentvolumes#100,persistentvolumeclaims#200"
      # Audit optimization to reduce overhead
      audit-log-maxbackup: "3" # Reduce from default 10
      audit-log-maxsize: "50" # Reduce from default 100MB
    # Scale API server resources significantly
    resources:
      requests:
        cpu: "1600m" # 8x increase from current 200m
        memory: "4Gi" # 8x increase from current 512Mi
      limits:
        cpu: "3200m" # Allow bursting
        memory: "8Gi" # Generous memory limit
  network:
    cni:
      name: none
    podSubnets:
      - 10.42.0.0/16
    serviceSubnets:
      - 10.69.0.0/16
  controllerManager:
    extraArgs:
      kube-api-qps: "500" # Increase API calls per second
      kube-api-burst: "1000" # Allow burst API calls
    resources:
      requests:
        cpu: "200m" # Increase from default 100m
        memory: "512Mi" # Increase from default 256Mi
      limits:
        cpu: "1000m"
        memory: "2Gi"
  proxy:
    disabled: true
  # etcd tuning
  # Increased timeouts to handle storage latency (e.g., ZFS-backed VMs with txg sync delays)
  # These settings make etcd more tolerant of brief I/O latency spikes
  etcd:
    extraArgs:
      # Auto-compaction
      auto-compaction-mode: "periodic"
      auto-compaction-retention: "12h"
      # Optimize backend quotas and request sizes
      quota-backend-bytes: "4294967296" # 4GB (conservative)
      max-request-bytes: "16777216" # 16MB for large CRD schemas
      # Timeout settings to reduce timeout errors under load and storage latency
      # Increase heartbeat interval to reduce coordination overhead
      heartbeat-interval: "250" # 250ms (default 100ms) - more stable under load
      # Increase election timeout to prevent premature leader elections during I/O latency spikes
      election-timeout: "5000" # 5000ms (default 1000ms) - 20x heartbeat-interval for stability
      # Enable initial election tick advance for faster startup
      initial-election-tick-advance: "true"
      # Metrics
      metrics: "basic"
      listen-metrics-urls: "http://0.0.0.0:2381"
---
# OOM Controller tuning - less aggressive triggering to prevent false positives
# during high pod count startup scenarios (e.g., 80+ pods on a node)
# Default: memory_full_avg10 > 12.0 && d_memory_full_avg10 > 0.0 && time_since_trigger > duration("500ms")
# Raised threshold to 25% and 60s cooldown with a 30s sample interval to allow Cilium to complete BPF map initialization
apiVersion: v1alpha1
kind: OOMConfig
triggerExpression: 'memory_full_avg10 > 25.0 && d_memory_full_avg10 > 0.0 && time_since_trigger > duration("60s")'
sampleInterval: 30s
