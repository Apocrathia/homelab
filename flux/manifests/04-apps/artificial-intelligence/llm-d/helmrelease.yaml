---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: llm-d-modelservice
  namespace: llm-d
spec:
  interval: 1h
  timeout: 15m
  chart:
    spec:
      chart: llm-d-modelservice
      # renovate: datasource=helm registryUrl=https://llm-d-incubation.github.io/llm-d-modelservice depName=llm-d-modelservice
      version: "v0.3.15"
      sourceRef:
        kind: HelmRepository
        name: llm-d-modelservice
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    # ModelService configuration
    # See https://llm-d.ai/docs/architecture/Components/modelservice for details
    #
    # This deployment uses inference-sim, which simulates inference behavior
    # inference-sim is designed for development/testing and doesn't perform actual inference
    # It doesn't require GPU resources and simulates the API without running models
    #
    # REQUIRED: Model artifacts configuration
    modelArtifacts:
      # Model name in format: namespace/modelId (used in OpenAI requests)
      # Using inference-sim identifier since this is a CPU-only simulator deployment
      name: "llm-d/inference-sim"
      # Model URI - supported formats:
      #   hf://model/name - Hugging Face model
      #   pvc://pvc_name/path/to/model - Model on existing PVC
      #   oci:// - OCI image (not yet fully supported)
      uri: "hf://meta-llama/Llama-3.2-1B"
      # Size of emptyDir volume for model download
      # Llama-3.2-1B requires ~2-3Gi with BFloat16, using 6Gi for safety margin
      # Note: inference-sim may not actually load the model, but space is reserved
      size: "1Gi"
      # Secret name containing HF_TOKEN for private Hugging Face models
      # The 1Password Item secret will be created as 'llm-d-secrets' in the namespace
      # IMPORTANT: The 1Password field name must be exactly 'HF_TOKEN' (uppercase)
      # The ModelService chart expects the Kubernetes secret key to be 'HF_TOKEN'
      # 1Password field names map directly to Kubernetes secret keys, so case matters
      authSecretName: llm-d-secrets
      # Mount path for model volume
      mountPath: /model-cache
      # Labels for pods (must match InferencePool matchLabels if using Gateway API)
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: inference-sim

    # Accelerator configuration
    # Set to "cpu" to prevent chart from injecting GPU resource requests
    # The chart defaults to "nvidia" if not set, which adds nvidia.com/gpu resources
    # Setting type to "cpu" makes acceleratorResource return empty string, preventing GPU injection
    accelerator:
      type: cpu

    # Routing configuration
    routing:
      # Port the service listens on (matches InferencePool targetPortNumber)
      servicePort: 8000
      # Routing proxy sidecar configuration
      proxy:
        enabled: true
        # Sidecar image for P/D disaggregation routing
        # renovate: datasource=docker depName=ghcr.io/llm-d/llm-d-routing-sidecar
        image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
        # Port inference-sim decode container listens on (proxy forwards to this)
        targetPort: 8200
        # Connector type: nixl, nixlv2
        connector: nixlv2
        # Logging configuration
        zapEncoder: json
        zapLogLevel: debug

    # Decode pod configuration (handles token generation)
    decode:
      create: true
      # Replicas: one per node for DaemonSet-like behavior
      # Note: Tensor parallelism requires LeaderWorkerSet size >= subGroupSize
      # The chart sets size=replicas, so we can't use tensor parallelism with this chart
      # Using data parallelism instead (each pod has full model)
      replicas: 4
      subGroupPolicy:
        subGroupSize: 1
      # Optional: Add custom anti-affinity via extraConfig if pods don't distribute evenly
      # This ensures decode pods prefer different nodes from other decode pods
      # extraConfig:
      #   affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #         - weight: 100
      #           podAffinityTerm:
      #             labelSelector:
      #               matchLabels:
      #                 llm-d.ai/role: decode
      #             topologyKey: kubernetes.io/hostname
      # Container configuration
      containers:
        - name: inference-sim
          # renovate: datasource=docker depName=ghcr.io/llm-d/llm-d-inference-sim
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.3.0
          # Command type: vllmServe, imageDefault, or custom
          modelCommand: imageDefault
          # Mount model volume
          mountModelVolume: true
          # Resource requirements (CPU-only mode with inference-sim)
          # inference-sim is designed for CPU-only workloads and doesn't require GPU
          resources:
            requests:
              # CPU-only mode - no GPU resources needed
              # Minimal requests for scheduling
              cpu: "500m"
              memory: "2Gi"
            limits:
              cpu: "4"
              memory: "16Gi"
          # Ports exposed by container
          ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP

      # Monitoring configuration for decode pods
      monitoring:
        podmonitor:
          enabled: true
          portName: "metrics"
          path: "/metrics"
          interval: "30s"
          labels: {}

    # Prefill pod configuration (handles prompt processing)
    # Prefill handles initial prompt processing (compute-intensive)
    # Decode handles token generation (memory bandwidth-intensive)
    prefill:
      create: true
      # Replicas: one per node for DaemonSet-like behavior
      replicas: 4
      subGroupPolicy:
        subGroupSize: 1
      # Optional: Add custom anti-affinity via extraConfig if pods don't distribute evenly
      # This ensures prefill pods prefer different nodes from other prefill pods
      # extraConfig:
      #   affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #         - weight: 100
      #           podAffinityTerm:
      #             labelSelector:
      #               matchLabels:
      #                 llm-d.ai/role: prefill
      #             topologyKey: kubernetes.io/hostname
      containers:
        - name: inference-sim
          # renovate: datasource=docker depName=ghcr.io/llm-d/llm-d-inference-sim
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.3.0
          modelCommand: imageDefault
          mountModelVolume: true
          resources:
            requests:
              # CPU-only mode - inference-sim doesn't require GPU resources
              # Minimal requests for scheduling
              cpu: "500m"
              memory: "2Gi"
            limits:
              cpu: "4"
              memory: "16Gi"
          ports:
            - containerPort: 8000
              name: metrics
              protocol: TCP

      # Monitoring configuration for prefill pods
      monitoring:
        podmonitor:
          enabled: true
          portName: "metrics"
          path: "/metrics"
          interval: "30s"
          labels: {}

    # Multi-node configuration (uses LeaderWorkerSet instead of Deployment)
    # Set to true for multi-node inference with data parallelism
    multinode: true

    # Optional: Fast Model Actuation (FMA) requester
    requester:
      enable: true

    # Optional: Dynamic Resource Allocation (DRA)
    dra:
      enabled: false
