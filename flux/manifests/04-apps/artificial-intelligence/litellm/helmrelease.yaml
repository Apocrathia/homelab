---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: litellm
  namespace: litellm
spec:
  interval: 1h
  timeout: 15m
  chart:
    spec:
      chart: litellm-helm
      # renovate: datasource=docker depName=ghcr.io/berriai/litellm-helm
      version: "0.1.832"
      sourceRef:
        kind: HelmRepository
        name: litellm
        namespace: flux-system

  # Pull all sensitive configuration from secret
  valuesFrom:
    - kind: Secret
      name: litellm-secrets
      valuesKey: master-key
      targetPath: proxy_config.general_settings.master_key
    - kind: Secret
      name: litellm-secrets
      valuesKey: master-key
      targetPath: envVars.LITELLM_MASTER_KEY
    - kind: Secret
      name: litellm-secrets
      valuesKey: username
      targetPath: proxy_config.general_settings.ui_username
    - kind: Secret
      name: litellm-secrets
      valuesKey: password
      targetPath: proxy_config.general_settings.ui_password
    - kind: Secret
      name: litellm-secrets
      valuesKey: anthropic-api-key
      targetPath: envVars.ANTHROPIC_API_KEY
    - kind: Secret
      name: litellm-secrets
      valuesKey: ollama-api-base
      targetPath: envVars.OLLAMA_API_BASE
    - kind: Secret
      name: litellm-secrets
      valuesKey: ollama-openai-api-base
      targetPath: envVars.OLLAMA_OPENAI_API_BASE
    - kind: Secret
      name: litellm-secrets
      valuesKey: openai-api-key
      targetPath: envVars.OPENAI_API_KEY
    - kind: Secret
      name: litellm-secrets
      valuesKey: vector-store-api-key
      targetPath: envVars.VECTOR_STORE_API_KEY
    - kind: Secret
      name: litellm-secrets
      valuesKey: redis-password
      targetPath: global.redis.password
    - kind: Secret
      name: litellm-secrets
      valuesKey: redis-password
      targetPath: redis.auth.password
    - kind: Secret
      name: litellm-secrets
      valuesKey: homeassistant-token
      targetPath: envVars.HOMEASSISTANT_TOKEN
    - kind: Secret
      name: litellm-secrets
      valuesKey: vector-store-id
      targetPath: envVars.VECTOR_STORE_ID

  values:
    # Allow ECR-hosted Bitnami images (they're mirrors of Docker Hub)
    global:
      security:
        allowInsecureImages: true

    # Use the database-optimized image
    image:
      repository: ghcr.io/berriai/litellm-database
      pullPolicy: IfNotPresent
      # renovate: datasource=docker depName=ghcr.io/berriai/litellm-database
      tag: "main-v1.80.5-stable"

    # Replica configuration
    replicaCount: 1

    # Service configuration
    service:
      type: ClusterIP
      port: 4000

    # Ingress disabled - using Gateway API through authentik
    ingress:
      enabled: false

    # Resource limits
    resources:
      requests:
        cpu: 50m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi

    # Environment secrets
    environmentSecrets:
      - litellm-secrets

    # Use existing database (our CNPG cluster)
    db:
      useExisting: true
      deployStandalone: false
      endpoint: litellm-postgres-rw.litellm.svc.cluster.local:5432
      database: litellm
      secret:
        name: litellm-secrets
        usernameKey: username
        passwordKey: password

    # Disable built-in postgres since we use CNPG
    postgresql:
      enabled: false

    # Enable redis for caching
    redis:
      enabled: true
      auth:
        enabled: true
      image:
        registry: public.ecr.aws
        repository: bitnami/redis
        # renovate: datasource=docker depName=public.ecr.aws/bitnami/redis
        tag: "8.4.0"

    # Use the rbac.yaml service account
    serviceAccount:
      create: false
      name: litellm

    # Proxy configuration
    proxyConfigMap:
      create: true

    # Mount the ConfigMap containing model configuration
    volumes:
      - name: model-config
        configMap:
          name: litellm-config

    volumeMounts:
      - name: model-config
        mountPath: /tmp/model-config/
        readOnly: true

    # Additional environment variables
    envVars:
      CONFIG_FILE_PATH: "/etc/litellm/config.yaml"
      GUARDRAILS_AI_API_BASE: "http://guardrails.guardrails-ai.svc.cluster.local:8000"
      # OpenTelemetry configuration for tracing
      OTEL_EXPORTER: "otlp_grpc"
      OTEL_ENDPOINT: "http://alloy.alloy-system.svc.cluster.local:4317"
      OTEL_SERVICE_NAME: "litellm"

    proxy_config:
      # Include model configuration from separate file using relative path
      include:
        - ../../tmp/model-config/models.yaml

      # Override default model_list to prevent duplicate entries with placeholder keys
      model_list: []

      general_settings:
        # Database configuration
        store_model_in_db: true

        # Allow client-side credentials for vector store registration
        allow_client_side_credentials: true

        # Rate limiting
        default_max_tokens: 4096
        default_tpm_limit: 60000
        default_rpm_limit: 600

      litellm_settings:
        # Logging and monitoring via OpenTelemetry
        success_callback: ["otel"]
        failure_callback: ["otel"]

        # Cost tracking
        track_cost_per_user: true

        # Health checks
        health_check_interval: 300

        # Caching
        cache: true
        cache_params:
          type: redis
          namespace: "litellm"
          ttl: 3600
          max_connections: 50

    # Migration job configuration
    migrationJob:
      enabled: true
      retries: 3
      backoffLimit: 4
      ttlSecondsAfterFinished: 3600 # 1 hour - allows Helm to manage the job properly
      hooks:
        argocd:
          enabled: false
        helm:
          enabled: true # Auto-deletes old job before creating new one

    # Health check separation
    separateHealthApp: true
    separateHealthPort: 8081

    # Pod disruption budget
    pdb:
      enabled: true
      minAvailable: 1
