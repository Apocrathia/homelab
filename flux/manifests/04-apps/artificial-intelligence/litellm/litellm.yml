---
model_list:
  # OpenAI models
  - model_name: gpt-5
    litellm_params:
      model: gpt-5
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-5-nano
    litellm_params:
      model: gpt-5-nano
      api_key: os.environ/OPENAI_API_KEY
  - model_name: o3-mini
    litellm_params:
      model: o3-mini
      api_key: os.environ/OPENAI_API_KEY

  # Claude models
  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/claude-sonnet-4-5
      api_key: os.environ/ANTHROPIC_API_KEY
  - model_name: claude-4-5-haiku
    litellm_params:
      model: anthropic/claude-haiku-4-5
      api_key: os.environ/ANTHROPIC_API_KEY

  # Ollama models (local LLM server)
  - model_name: llama3.2
    litellm_params:
      model: ollama/llama3.2:3b
      api_base: os.environ/OLLAMA_API_BASE
  - model_name: gpt-oss
    litellm_params:
      model: ollama/gpt-oss:20b
      api_base: os.environ/OLLAMA_API_BASE
  - model_name: dolphin3
    litellm_params:
      model: ollama/dolphin3:8b
      api_base: os.environ/OLLAMA_API_BASE
  - model_name: deepseek-r1
    litellm_params:
      model: ollama/deepseek-r1:14b
      api_base: os.environ/OLLAMA_API_BASE
  - model_name: qwen3
    litellm_params:
      model: ollama/qwen3:8b
      api_base: os.environ/OLLAMA_API_BASE

  # Ollama embedding models
  - model_name: nomic-embed-text
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: os.environ/OLLAMA_API_BASE

  # llm-d models (Kubernetes-native inference serving with inference-sim)
  - model_name: inference-sim
    litellm_params:
      model: openai/llm-d/inference-sim
      api_base: http://llm-d-modelservice.llm-d.svc.cluster.local:8000

guardrails:
  - guardrail_name: "gibberish-guard"
    litellm_params:
      guardrail: guardrails_ai
      guard_name: "gibberish-guard"
      mode: "post_call"
      guardrails_ai_api_input_format: "llmOutput"
      api_base: os.environ/GUARDRAILS_AI_API_BASE
      default_on: true

vector_store_registry:
  - vector_store_name: "pgvector-litellm"
    litellm_params:
      custom_llm_provider: "pg_vector"
      vector_store_id: "09beee6f-ed62-47c6-a161-dea9018a5a40"
      vector_store_description: "PGVector vector store for LiteLLM"
      vector_store_metadata:
        api_base: "http://vector-store.litellm.svc.cluster.local:8000"
        api_key: os.environ/VECTOR_STORE_API_KEY

mcp_servers:
  gofetch:
    url: "http://mcp-gofetch-proxy.mcp-gofetch.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
  osv:
    url: "http://mcp-osv-vulnerability-scanner-proxy.mcp-osv.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
  mkp:
    url: "http://mcp-kubernetes-mcp-proxy.mcp-mkp.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
  grafana:
    url: "http://mcp-grafana-mcp-proxy.mcp-grafana.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
    # Forward Grafana auth headers from client
    extra_headers:
      - "X-Grafana-API-Key"
  searxng:
    url: "http://mcp-searxng-mcp-proxy.mcp-searxng.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
  github:
    url: "https://api.githubcopilot.com/mcp/"
    transport: "http"
    auth_type: "none"
    # Forward GitHub PAT from client
    extra_headers:
      - "Authorization"
  flux:
    url: "http://mcp-flux-mcp-proxy.mcp-flux.svc.cluster.local:8080/mcp"
    transport: "http"
    auth_type: "none"
  deepwiki:
    url: "https://mcp.deepwiki.com/mcp"
    transport: "http"
    auth_type: "none"
