---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: longhorn
  namespace: longhorn-system
spec:
  interval: 30m
  chart:
    spec:
      chart: longhorn
      # renovate: datasource=helm registryUrl=https://charts.longhorn.io depName=longhorn
      version: 1.10.1
      sourceRef:
        kind: HelmRepository
        name: longhorn
        namespace: flux-system
  maxHistory: 2
  # Add resource requests to longhorn-manager to get Burstable QoS
  # This prevents OOM killer from targeting it first during memory pressure
  postRenderers:
    - kustomize:
        patches:
          - target:
              kind: DaemonSet
              name: longhorn-manager
            patch: |
              - op: add
                path: /spec/template/spec/containers/0/resources
                value:
                  requests:
                    cpu: 100m
                    memory: 256Mi
                  limits:
                    cpu: 1000m
                    memory: 1Gi
  install:
    crds: Create
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    # Disable upgrade version check to allow hotfix image
    # See: https://github.com/longhorn/longhorn/releases/tag/v1.10.1
    upgradeVersionCheck: false

    # Use hotfixed manager image to fix crashes and replica auto-balance bugs
    image:
      longhorn:
        manager:
          tag: v1.10.1-hotfix-2

    defaultBackupStore:
      backupTarget: cifs://storage.services.apocrathia.com/Backup
      backupTargetCredentialSecret: longhorn-backup-target-secret
      pollInterval: 300
    defaultSettings:
      # Backup and restore settings
      restoreVolumeRecurringJobs: true
      backupCompressionMethod: gzip
      deletingConfirmationFlag: true

      # Reliability and stability settings
      autoSalvage: true # Automatically fix corrupted volumes
      autoDeletePodWhenVolumeDetachedUnexpectedly: true # Auto-delete pods when volumes detach unexpectedly

      # Replica settings for better stability
      defaultReplicaCount: "3" # Default replica count for new volumes
      replicaSoftAntiAffinity: true # Allow scheduling on nodes with healthy replicas
      replicaAutoBalance: "best-effort" # Auto-rebalance replicas when nodes are available

      # Timeout and cleanup settings
      staleReplicaTimeout: 300 # 5 minutes
      failedBackupTTL: "1440" # 24 hours
      backupExecutionTimeout: "30" # 30 minutes (increased to allow large volume backups to complete)
      backupConcurrentLimit: "10" # Increased to match recurring job concurrency (75 volumes need higher throughput)

      # Storage settings
      storageOverProvisioningPercentage: "150"
      storageMinimalAvailablePercentage: "25"
      defaultDataLocality: "best-effort" # Allow replicas on different nodes

      # Engine and replica settings
      engineReplicaTimeout: "30" # 30 seconds timeout
      concurrentReplicaRebuildPerNodeLimit: "3" # Limit concurrent rebuilds
      concurrentVolumeBackupRestorePerNodeLimit: "1" # Limit concurrent restores

      # Data integrity and recovery
      snapshotDataIntegrity: "fast-check" # Enable fast integrity checking
      fastReplicaRebuildEnabled: "true" # Enable fast rebuild using checksums

      # Cleanup settings
      autoCleanupSystemGeneratedSnapshot: true
      autoCleanupRecurringJobBackupSnapshot: false # Disabled to prevent snapshots from being deleted before backups complete
      removeSnapshotsDuringFilesystemTrim: true
      snapshotMaxCount: "5" # Limit snapshots per volume (default 250 is too high, prevents accumulation)

      # Node and disk settings
      disableSchedulingOnCordonedNode: true # Don't schedule on cordoned nodes
      replicaZoneSoftAntiAffinity: true # Prefer same zone for replicas
      replicaDiskSoftAntiAffinity: true # Allow same disk for replicas

      # Volume creation settings
      allowVolumeCreationWithDegradedAvailability: true # Allow creation without all replicas
      allowRecurringJobWhileVolumeDetached: true # Allow jobs on detached volumes

      # Orphaned resource cleanup
      # Valid values: replica-data, instance, or replica-data;instance (semicolon-separated)
      orphanResourceAutoDeletion: "replica-data;instance" # Auto-delete orphaned replicas and instances
      orphanResourceAutoDeletionGracePeriod: "600" # 10 minutes

    # Persistence settings for better reliability
    persistence:
      defaultClass: true
      defaultFsType: ext4
      defaultClassReplicaCount: 3 # Match the defaultReplicaCount
      defaultDataLocality: "best-effort"
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      migratable: false
      disableRevisionCounter: "true"
      unmapMarkSnapChainRemoved: ignored
      dataEngine: v1
      # Configure recurring job selector to automatically label all new volumes with the default group
      # This ensures new volumes receive all recurring jobs (backup, snapshot-delete, etc.) from the default group
      recurringJobSelector:
        enable: true
        jobList: '[{"name":"default","isGroup":true}]'

    longhornUI:
      replicas: 2

    ingress:
      enabled: false

    # CSI settings for better reliability
    csi:
      attacherReplicaCount: 2 # Reduce from default 3
      provisionerReplicaCount: 2 # Reduce from default 3
      resizerReplicaCount: 2 # Reduce from default 3
      snapshotterReplicaCount: 2 # Reduce from default 3

    #  For Kubernetes < v1.25, if your cluster enables Pod Security Policy admission controller,
    #  set this to `true` to ship longhorn-psp which allow privileged Longhorn pods to start
    enablePSP: false

    ## Specify override namespace, specifically this is useful for using longhorn as sub-chart
    ## and its release namespace is not the `longhorn-system`
    namespaceOverride: ""

    # Annotations to add to the Longhorn Manager DaemonSet Pods. Optional.
    annotations: {}

    serviceAccount:
      # Annotations to add to the service account
      annotations: {}

    metrics:
      serviceMonitor:
        enabled: true
        interval: "30s"
        scrapeTimeout: "10s"
        additionalLabels:
          app: longhorn
          component: storage
        annotations:
          prometheus.io/scrape: "true"
